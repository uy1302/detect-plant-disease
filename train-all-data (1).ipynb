{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import\n\n","metadata":{}},{"cell_type":"code","source":"import os\nimport os.path as osp\nimport time\nimport datetime\nimport random\nfrom PIL import Image\nimport numpy as np\nfrom torch.autograd import Variable\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\nfrom torch.autograd import Function\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nimport tensorflow as tf\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch.utils.data as data\nfrom torch.utils.data import random_split, TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\nimport logging\nfrom torchvision import datasets,transforms, models\nimport tqdm\nimport glob\nimport cv2\nimport tensorflow_hub as hub\n%matplotlib inline\nuse_gpu = torch.cuda.is_available()\nif use_gpu:\n    print(\"Using CUDA\")","metadata":{"execution":{"iopub.status.busy":"2023-10-06T10:55:23.379536Z","iopub.execute_input":"2023-10-06T10:55:23.379945Z","iopub.status.idle":"2023-10-06T10:55:36.621971Z","shell.execute_reply.started":"2023-10-06T10:55:23.379919Z","shell.execute_reply":"2023-10-06T10:55:36.620922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Version \", tf.__version__)\nprint(\"Eager mode:\", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\",\"available\" if tf.test.is_gpu_available() else\"Not Available\")","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:57:33.082756Z","iopub.execute_input":"2023-09-08T09:57:33.083547Z","iopub.status.idle":"2023-09-08T09:57:38.823882Z","shell.execute_reply.started":"2023-09-08T09:57:33.083511Z","shell.execute_reply":"2023-09-08T09:57:38.822822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"markdown","source":"## Tomato","metadata":{}},{"cell_type":"code","source":"source_tomato = '/kaggle/input/plant-village/PlantVillage'\nos.listdir(source_tomato)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:57:38.824924Z","iopub.execute_input":"2023-09-08T09:57:38.825289Z","iopub.status.idle":"2023-09-08T09:57:38.846653Z","shell.execute_reply.started":"2023-09-08T09:57:38.825258Z","shell.execute_reply":"2023-09-08T09:57:38.845435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nsource_folder= source_tomato\ndestination_folder=os.path.join(\"/kaggle/working/\",\"Tomato_dataset\")\nif not os.path.exists(destination_folder):\n    os.mkdir(destination_folder)\n\n    subdirectories = os.listdir(source_folder)\n    for subdirectory in subdirectories:\n        if (subdirectory[:6]=='Tomato'):\n            subdirectory_path = os.path.join(source_folder, subdirectory)\n            shutil.copytree(subdirectory_path, os.path.join(destination_folder, subdirectory))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:57:38.849751Z","iopub.execute_input":"2023-09-08T09:57:38.850215Z","iopub.status.idle":"2023-09-08T09:59:42.903094Z","shell.execute_reply.started":"2023-09-08T09:57:38.850179Z","shell.execute_reply":"2023-09-08T09:59:42.90195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove File","metadata":{}},{"cell_type":"code","source":"files2 = [file for file in os.listdir('/kaggle/working/cornCommon_Rust')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working/Potato_dataset'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Potato","metadata":{}},{"cell_type":"code","source":"source_potato = '/kaggle/input/potato-disease-leaf-datasetpld/PLD_3_Classes_256'\nos.listdir(source_potato)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:59:42.904702Z","iopub.execute_input":"2023-09-08T09:59:42.905078Z","iopub.status.idle":"2023-09-08T09:59:42.920996Z","shell.execute_reply.started":"2023-09-08T09:59:42.905044Z","shell.execute_reply":"2023-09-08T09:59:42.920021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_folder = source_potato\ndestination_folder = os.path.join(\"/kaggle/working/\",\"Potato_dataset\")\nif not os.path.exists(destination_folder):\n    os.mkdir(destination_folder)\n\n    subdirectories = os.listdir(source_folder)\n    for subdirectory in subdirectories:\n        subdirectory_path = os.path.join(source_folder, subdirectory)\n        subs = os.listdir(subdirectory_path)\n        for sub in subs:\n#             print(sub)\n            sub_path = os.path.join(subdirectory_path, sub)\n            sub = 'Potato_' + sub\n            destination = os.path.join(destination_folder, subdirectory)\n#             print(destination)\n            shutil.copytree(sub_path, os.path.join(destination, sub))\n#         subdirectory = 'Corn_' + subdirectory\n#         shutil.copytree(subdirectory_path, os.path.join(destination_folder, subdirectory))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:59:42.922377Z","iopub.execute_input":"2023-09-08T09:59:42.922805Z","iopub.status.idle":"2023-09-08T10:00:14.480419Z","shell.execute_reply.started":"2023-09-08T09:59:42.922771Z","shell.execute_reply":"2023-09-08T10:00:14.479271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Corn","metadata":{}},{"cell_type":"code","source":"source_corn = '/kaggle/input/corn-or-maize-leaf-disease-dataset/data'\nos.listdir(source_corn)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:00:14.483085Z","iopub.execute_input":"2023-09-08T10:00:14.483791Z","iopub.status.idle":"2023-09-08T10:00:14.499561Z","shell.execute_reply.started":"2023-09-08T10:00:14.483751Z","shell.execute_reply":"2023-09-08T10:00:14.498554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nsource_folder= source_corn\ndestination_folder=os.path.join(\"/kaggle/working/\",\"Corn_dataset\")\nif not os.path.exists(destination_folder):\n    os.mkdir(destination_folder)\n\n    subdirectories = os.listdir(source_folder)\n    for subdirectory in subdirectories:\n        subdirectory_path = os.path.join(source_folder, subdirectory)\n        subdirectory = 'Corn_' + subdirectory\n        shutil.copytree(subdirectory_path, os.path.join(destination_folder, subdirectory))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:00:14.500732Z","iopub.execute_input":"2023-09-08T10:00:14.501029Z","iopub.status.idle":"2023-09-08T10:00:50.670063Z","shell.execute_reply.started":"2023-09-08T10:00:14.500986Z","shell.execute_reply":"2023-09-08T10:00:50.668845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RICE","metadata":{}},{"cell_type":"code","source":"source_rice = '/kaggle/input/rice-diseases-image-dataset/LabelledRice/Labelled'\nos.listdir(source_rice)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:00:50.67351Z","iopub.execute_input":"2023-09-08T10:00:50.673928Z","iopub.status.idle":"2023-09-08T10:00:50.691607Z","shell.execute_reply.started":"2023-09-08T10:00:50.67389Z","shell.execute_reply":"2023-09-08T10:00:50.690452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nsource_folder= source_rice\ndestination_folder=os.path.join(\"/kaggle/working/\",\"Rice_dataset\")\nif not os.path.exists(destination_folder):\n    os.mkdir(destination_folder)\n\n    subdirectories = os.listdir(source_folder)\n    for subdirectory in subdirectories:\n        subdirectory_path = os.path.join(source_folder, subdirectory)\n        subdirectory = 'Rice_' + subdirectory\n        shutil.copytree(subdirectory_path, os.path.join(destination_folder, subdirectory))","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:00:50.69337Z","iopub.execute_input":"2023-09-08T10:00:50.693774Z","iopub.status.idle":"2023-09-08T10:03:25.827426Z","shell.execute_reply.started":"2023-09-08T10:00:50.693738Z","shell.execute_reply":"2023-09-08T10:03:25.826357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = []\nfor i in data:\n    l.append(labels[i[1]])\nsns.set_style('dark')\nsns.countplot(l)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transform","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nimage_size = (224,224)\nchannels = 3","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:04:37.310701Z","iopub.execute_input":"2023-09-08T10:04:37.31109Z","iopub.status.idle":"2023-09-08T10:04:37.315971Z","shell.execute_reply.started":"2023-09-08T10:04:37.311058Z","shell.execute_reply":"2023-09-08T10:04:37.315038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_corn = tf.keras.utils.image_dataset_from_directory(\n    directory = '/kaggle/working/Corn_dataset',\n    color_mode ='rgb',\n    batch_size = batch_size,\n    image_size = image_size,\n    shuffle = True\n)\ndata_tomato = tf.keras.utils.image_dataset_from_directory(\n    directory = '/kaggle/working/Tomato_dataset',\n    color_mode ='rgb',\n    batch_size = batch_size,\n    image_size = image_size,\n    shuffle = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dirs_in=data_corn.class_names\ndirs_in","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_corn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch,labels in data_corn.take(1):\n    print(f\"Shape of per batch: {batch.shape} Type: {type(batch)}\")\n    print(f\"Shape of labels: {labels.shape} Type: {type(labels)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Data","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\n\ndef split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, batch_size=8, shuffle=True):\n    num_samples = len(dataset)\n    num_train = int(train_ratio * num_samples)\n    num_val = int(val_ratio * num_samples)\n    num_test = num_samples - num_train - num_val\n    \n    train_data, val_data, test_data = random_split(dataset, [num_train, num_val, num_test])\n    \n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n    val_loader = DataLoader(val_data, batch_size=batch_size)\n    test_loader = DataLoader(test_data, batch_size=batch_size)\n    \n    return train_loader, val_loader, test_loader\n\n\n# Use the split_dataset function\ntrain_dataloader_corn, val_dataloader_corn, test_dataloader_corn = split_dataset(data_corn)\ntrain_dataloader_tomato, val_dataloader_tomato, test_dataloader_tomato = split_dataset(data_tomato)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_dataloader_corn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {...}\ndata_dir = \"path_to_your_data_directory\"\n\n# Create the original ImageFolder dataset\noriginal_dataset = ImageFolder(root=data_dir, transform=data_transforms['train'])\n\n# Assuming you want to convert a subset of the dataset into an ImageFolder dataset\nsubset_indices = [0, 1, 2, 3, ...]  # Replace with your desired subset indices\nsubset_dataset = ImageFolder(root=data_dir, transform=data_transforms['train'], loader=lambda x: original_dataset.loader(x))\nsubset_dataset = Subset(subset_dataset, subset_indices)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(dataset,train_size=0.8,test_size=0.1,val_size=0.1,shuffle=True,shuffle_size=10000):\n    if shuffle:\n        dataset=dataset.shuffle(shuffle_size,seed=20)\n    train_data=dataset.take(int(train_size*len(dataset)))\n    test_data=dataset.skip(int(train_size*len(dataset))).take(int(test_size*len(dataset)))\n    val_data=dataset.skip(int((train_size+test_size)*len(dataset)))\n    \n    return train_data,test_data,val_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corn_train,corn_test,corn_val = split_dataset(data_corn)\nlen(corn_train),len(corn_test),len(corn_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tomato_train,tomato_test,tomato_val = split_dataset(data_tomato)\nlen(tomato_train),len(tomato_test),len(tomato_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corn_train=corn_train.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ncorn_test=corn_test.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ncorn_val=corn_val.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tomato_train = tomato_train.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ntomato_test = tomato_test.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ntomato_val = tomato_val.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tomato_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(corn_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_to_if(corn_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data RICE\nimport os\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.utils.data import random_split\n\n# Load your existing ImageFolder dataset\nimagefolder_tomato = ImageFolder(root=\"/kaggle/working/Rice_dataset\", transform=data_transforms[TRAIN])\n\n# Split the dataset\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\nnum_samples = len(imagefolder_tomato)\nnum_train = int(train_ratio * num_samples)\nnum_val = int(val_ratio * num_samples)\nnum_test = num_samples - num_train - num_val\n\ntrain_data, val_data, test_data = random_split(imagefolder_tomato, [num_train, num_val, num_test])\n\n# Create output directories\noutput_dir = \"/kaggle/working/Potato_dataset\"\ntrain_output_dir = os.path.join(output_dir, \"Training\")\nval_output_dir = os.path.join(output_dir, \"Validation\")\ntest_output_dir = os.path.join(output_dir, \"Testing\")\n\nos.makedirs(train_output_dir, exist_ok=True)\nos.makedirs(val_output_dir, exist_ok=True)\nos.makedirs(test_output_dir, exist_ok=True)\n\n# Save images to output directories\nto_pil = ToPILImage()\ni = 1\nfor data, label in train_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(train_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in val_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(val_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in test_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(test_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:06:22.294694Z","iopub.execute_input":"2023-09-08T10:06:22.295216Z","iopub.status.idle":"2023-09-08T10:11:23.075576Z","shell.execute_reply.started":"2023-09-08T10:06:22.295166Z","shell.execute_reply":"2023-09-08T10:11:23.073488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/Tomato_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.utils.data import random_split\n\n# Load your existing ImageFolder dataset\nimagefolder_tomato = ImageFolder(root=\"/kaggle/working/Tomato_dataset\", transform=data_transforms[TRAIN])\n\n# Split the dataset\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\nnum_samples = len(imagefolder_tomato)\nnum_train = int(train_ratio * num_samples)\nnum_val = int(val_ratio * num_samples)\nnum_test = num_samples - num_train - num_val\n\ntrain_data, val_data, test_data = random_split(imagefolder_tomato, [num_train, num_val, num_test])\n\n# Create output directories\noutput_dir = \"/kaggle/working/Potato_dataset\"\ntrain_output_dir = os.path.join(output_dir, \"Training\")\nval_output_dir = os.path.join(output_dir, \"Validation\")\ntest_output_dir = os.path.join(output_dir, \"Testing\")\n\nos.makedirs(train_output_dir, exist_ok=True)\nos.makedirs(val_output_dir, exist_ok=True)\nos.makedirs(test_output_dir, exist_ok=True)\n\n# Save images to output directories\nto_pil = ToPILImage()\ni = 1\nfor data, label in train_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(train_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in val_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(val_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in test_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(test_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:11:23.077661Z","iopub.execute_input":"2023-09-08T10:11:23.079149Z","iopub.status.idle":"2023-09-08T10:12:41.047658Z","shell.execute_reply.started":"2023-09-08T10:11:23.079111Z","shell.execute_reply":"2023-09-08T10:12:41.045082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToPILImage\nfrom torch.utils.data import random_split\n\n# Load your existing ImageFolder dataset\nimagefolder_tomato = ImageFolder(root=\"/kaggle/working/Corn_dataset\", transform=data_transforms[TRAIN])\n\n# Split the dataset\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\nnum_samples = len(imagefolder_tomato)\nnum_train = int(train_ratio * num_samples)\nnum_val = int(val_ratio * num_samples)\nnum_test = num_samples - num_train - num_val\n\ntrain_data, val_data, test_data = random_split(imagefolder_tomato, [num_train, num_val, num_test])\n\n# Create output directories\noutput_dir = \"/kaggle/working/Potato_dataset\"\ntrain_output_dir = os.path.join(output_dir, \"Training\")\nval_output_dir = os.path.join(output_dir, \"Validation\")\ntest_output_dir = os.path.join(output_dir, \"Testing\")\n\nos.makedirs(train_output_dir, exist_ok=True)\nos.makedirs(val_output_dir, exist_ok=True)\nos.makedirs(test_output_dir, exist_ok=True)\n\n# Save images to output directories\nto_pil = ToPILImage()\ni = 1\nfor data, label in train_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(train_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in val_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(val_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)\ni = 1\nfor data, label in test_data:\n    class_name =imagefolder_tomato.classes[label]\n    class_output_dir = os.path.join(test_output_dir, class_name)\n    os.makedirs(class_output_dir, exist_ok=True)\n    image = to_pil(data)\n    image.save(os.path.join(class_output_dir, f\"image_{i}.jpg\"))\n    i += 1\nprint(i)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:12:41.05272Z","iopub.execute_input":"2023-09-08T10:12:41.053283Z","iopub.status.idle":"2023-09-08T10:13:06.578116Z","shell.execute_reply.started":"2023-09-08T10:12:41.05324Z","shell.execute_reply":"2023-09-08T10:13:06.577064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = \"/kaggle/working/Tomato_Folder/Validation/Tomato_Early_blight\"\n\n# List the files in the folder\nfile_list = os.listdir(folder_path)\n\n# Calculate the number of files\nnum_files = len(file_list)\nprint(num_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len('/kaggle/working/Tomato_Folder/Training')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file.remove(\"/kaggle/working/file.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/potato-disease-leaf-datasetpld/PLD_3_Classes_256'\nTRAIN = 'Training'\nVAL = 'Validation'\nTEST = 'Testing'\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:05:01.751701Z","iopub.execute_input":"2023-09-08T10:05:01.752091Z","iopub.status.idle":"2023-09-08T10:05:01.761673Z","shell.execute_reply.started":"2023-09-08T10:05:01.752059Z","shell.execute_reply":"2023-09-08T10:05:01.760443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\ndata_dir = '/kaggle/working/Potato_dataset'\nTRAIN = 'Training'\nVAL = 'Validation'\nTEST = 'Testing'\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.Resize((224,224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])\n}\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(data_dir, x), \n        transform=data_transforms[x]\n    )\n    for x in [TRAIN, VAL, TEST]\n}\n# new_class_names = ['Potato_Early_Blight', 'Potato_Healthy', 'Potato_Late_Blight']\n# for x in [TRAIN, VAL, TEST]:\n#     image_datasets[x].classes = new_class_names\n# type(image_datasets[TRAIN])\n# corn_dir = '/kaggle/working/Corn_Folder'\n# tomato_dir = '/kaggle/working/Tomato_Folder'\n# corn_datasets = {\n#     x: datasets.ImageFolder(\n#         os.path.join(corn_dir, x), \n#         transform=data_transforms[x]\n#     )\n#     for x in [TRAIN, VAL, TEST]\n# }\n# tomato_datasets = {\n#     x: datasets.ImageFolder(\n#         os.path.join(tomato_dir, x), \n#         transform=data_transforms[x]\n#     )\n#     for x in [TRAIN, VAL, TEST]\n# }\n# image_datasets[TRAIN] += corn_datasets[TRAIN]\n# image_datasets[TRAIN] = ConcatDataset([image_datasets[TRAIN], corn_datasets[TRAIN]])\n\n# image_datasets[TRAIN] += corn_datasets[TRAIN]\n# image_datasets[TRAIN] += tomato_datasets[TRAIN]\n# print(type(image_datasets))\n# image_datasets[TRAIN] += corn_train\n# type(image_datasets[TRAIN])\n# class_names = image_datasets[TRAIN].classes\n# len(class_names)  \ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=8,\n        shuffle=True, num_workers=4\n    )\n    for x in [TRAIN, VAL, TEST]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n\nfor x in [TRAIN, VAL, TEST]:\n    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n\nclass_names = image_datasets[TRAIN].classes\n\nprint(image_datasets[TEST].classes)   ","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:13:06.580571Z","iopub.execute_input":"2023-09-08T10:13:06.580927Z","iopub.status.idle":"2023-09-08T10:13:06.728488Z","shell.execute_reply.started":"2023-09-08T10:13:06.580893Z","shell.execute_reply":"2023-09-08T10:13:06.72744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:14:20.812167Z","iopub.execute_input":"2023-09-08T10:14:20.812529Z","iopub.status.idle":"2023-09-08T10:14:20.819404Z","shell.execute_reply.started":"2023-09-08T10:14:20.812499Z","shell.execute_reply":"2023-09-08T10:14:20.818288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(0.1),\n#         transforms.ToTensor(),\n        transforms.Normalize(mean=[0.4884, 0.4551, 0.4170], std=[0.2256, 0.2210, 0.2214]),\n#         transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.Normalize(mean=[0.4884, 0.4551, 0.4170], std=[0.2256, 0.2210, 0.2214]),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    TRAIN: transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.RandomHorizontalFlip(0.1),\n        transforms.ToTensor(),  # Convert PIL Image to Tensor\n        transforms.Normalize(mean=[0.4884, 0.4551, 0.4170], std=[0.2256, 0.2210, 0.2214]),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),  # Convert PIL Image to Tensor\n        transforms.Normalize(mean=[0.4884, 0.4551, 0.4170], std=[0.2256, 0.2210, 0.2214]),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),  # Convert PIL Image to Tensor\n    ])\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for rice only\nfrom torch.utils.data import ConcatDataset\ndata_dir = '/kaggle/working/Rice_folder'\nTRAIN = 'Training'\nVAL = 'Validation'\nTEST = 'Testing'\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\n# data_transforms = {\n#     TRAIN: transforms.Compose([\n#         # Data augmentation is a good practice for the train set\n#         # Here, we randomly crop the image to 224x224 and\n#         # randomly flip it horizontally. \n#         transforms.RandomResizedCrop(224),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.ToTensor(),\n#     ]),\n#     VAL: transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#         transforms.ToTensor(),\n#     ]),\n#     TEST: transforms.Compose([\n#         transforms.Resize(256),\n#         transforms.CenterCrop(224),\n#         transforms.ToTensor(),\n#     ])\n# }\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(data_dir, x), \n        transform=data_transforms[x]\n    )\n    for x in [TRAIN, VAL, TEST]\n}\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=8,\n        shuffle=True, num_workers=4\n    )\n    for x in [TRAIN, VAL, TEST]\n}\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n\nfor x in [TRAIN, VAL, TEST]:\n    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n\nclass_names = image_datasets[TRAIN].classes\n\nprint(image_datasets[TEST].classes)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(class_names[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloaders[TRAIN] = DataLoader(\n    ConcatDataset([corn_data, ]),\n    batch_size=batch_size,\n    shuffle=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def imshow(inp, title=None):\n#     inp = inp.numpy().transpose((1, 2, 0))\n#     plt.figure(figsize=(10, 10))\n#     plt.axis('off')\n#     plt.imshow(inp)\n#     if title is not None:\n#         plt.title(title)\n#     plt.pause(0.001)\n\n# def show_databatch(inputs, classes):\n#     out = torchvision.utils.make_grid(inputs)\n#     imshow(out, title=[class_names[x] for x in classes])\n\n# # Get a batch of training data\n# inputs, classes = next(iter(dataloaders[TRAIN]))\n# show_databatch(inputs, classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_name = class_names","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:14:50.70049Z","iopub.execute_input":"2023-09-08T10:14:50.700878Z","iopub.status.idle":"2023-09-08T10:14:50.705389Z","shell.execute_reply.started":"2023-09-08T10:14:50.700845Z","shell.execute_reply":"2023-09-08T10:14:50.704157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:14:52.336134Z","iopub.execute_input":"2023-09-08T10:14:52.336985Z","iopub.status.idle":"2023-09-08T10:14:52.344861Z","shell.execute_reply.started":"2023-09-08T10:14:52.33695Z","shell.execute_reply":"2023-09-08T10:14:52.343612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def imshow_with_title(inp, title):\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.figure()\n    plt.imshow(inp)\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\ndef show_databatch_with_titles(inputs, class_names, cols=4):\n    batch_size, num_channels, height, width = inputs.shape\n    rows = int(np.ceil(batch_size / cols))\n    fig, axes = plt.subplots(rows, cols, figsize=(9, 9))\n    for ax in axes.flatten():\n        ax.axis('off')\n    \n    for i in range(batch_size):\n        row = i // cols\n        col = i % cols\n        ax = axes[row,col]\n        img = inputs[i].numpy().transpose((1, 2, 0))\n        ax.imshow(img)\n#         print(class_names[i].item())\n        ax.set_title(class_name[class_names[i].item()], fontsize = 7.5)\n#         print(class_names[i])\n    \n    plt.tight_layout()\n    plt.show()\n\ninputs, classes = next(iter(dataloaders[TRAIN]))\n\nshow_databatch_with_titles(inputs, classes)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:16:30.860555Z","iopub.execute_input":"2023-09-08T10:16:30.860949Z","iopub.status.idle":"2023-09-08T10:16:32.217055Z","shell.execute_reply.started":"2023-09-08T10:16:30.860909Z","shell.execute_reply":"2023-09-08T10:16:32.215983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_model(vgg, num_images=6):\n    was_training = vgg.training\n    \n    # Set model for evaluation\n    vgg.train(False)\n    vgg.eval() \n    \n    images_so_far = 0\n\n    for i, data in enumerate(dataloaders[TEST]):\n        inputs, labels = data\n        size = inputs.size()[0]\n        inputs, labels = inputs.cuda(), labels.cuda()\n#         if use_gpu:\n#             inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n#         else:\n#             inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n        \n        outputs = vgg(inputs)\n        \n        _, preds = torch.max(outputs.data, 1)\n        predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n        \n        print(\"Ground truth:\")\n        show_databatch_with_titles(inputs.data.cpu(), labels.data.cpu())\n        print(\"Prediction:\")\n        show_databatch_with_titles(inputs.data.cpu(), predicted_labels)\n        \n        del inputs, labels, outputs, preds, predicted_labels\n        torch.cuda.empty_cache()\n        \n        images_so_far += size\n        if images_so_far >= num_images:\n            break\n        \n    vgg.train(mode=was_training) # Revert model back to original training state","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:16:38.529144Z","iopub.execute_input":"2023-09-08T10:16:38.529565Z","iopub.status.idle":"2023-09-08T10:16:38.540357Z","shell.execute_reply.started":"2023-09-08T10:16:38.529532Z","shell.execute_reply":"2023-09-08T10:16:38.539051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def eval_model(vgg, criterion):\n    since = time.time()\n    avg_loss = 0\n    avg_acc = 0\n    loss_test = 0\n    acc_test = 0\n    \n    test_batches = len(dataloaders[TEST])\n    print(\"Evaluating model\")\n    print('-' * 10)\n    \n    for i, data in enumerate(dataloaders[TEST]):\n        if i % 100 == 0:\n            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n\n        vgg.train(False)\n        vgg.eval()\n        inputs, labels = data\n        inputs, labels = inputs.cuda(), labels.cuda()\n#         if use_gpu:\n#             inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n#         else:\n#             inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n\n        outputs = vgg(inputs)\n\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n\n        loss_test += loss.data\n        acc_test += torch.sum(preds == labels.data)\n\n        del inputs, labels, outputs, preds\n        torch.cuda.empty_cache()\n        \n    avg_loss = loss_test / dataset_sizes[TEST]\n    avg_acc = acc_test / dataset_sizes[TEST]\n    \n    elapsed_time = time.time() - since\n    print()\n    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n    print('-' * 10)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:16:49.735871Z","iopub.execute_input":"2023-09-08T10:16:49.73685Z","iopub.status.idle":"2023-09-08T10:16:49.748602Z","shell.execute_reply.started":"2023-09-08T10:16:49.736815Z","shell.execute_reply":"2023-09-08T10:16:49.747335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load the pretrained model from pytorch\nvgg16 = models.vgg16_bn()\nvgg16.load_state_dict(torch.load(\"/kaggle/input/vgg16bn/vgg16_bn.pth\"))\nprint(vgg16.classifier[6].out_features) # 1000 \n\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nfeatures = list(vgg16.classifier.children())[:-1] # Remove last layer\nfeatures.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*features) # Replace the model classifier\nprint(vgg16)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:19:05.748617Z","iopub.execute_input":"2023-09-08T10:19:05.748984Z","iopub.status.idle":"2023-09-08T10:19:08.500207Z","shell.execute_reply.started":"2023-09-08T10:19:05.748954Z","shell.execute_reply":"2023-09-08T10:19:08.497538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Load the pre-trained VGG19 model with batch normalization\nvgg19 = models.vgg19_bn(pretrained=True)\nvgg19.load_state_dict(torch.load(\"/kaggle/input/vgg19bn/vgg19_bn.pth\"))\n# Print the original number of output features in the classifier (usually 1000 for ImageNet)\nprint(vgg19.classifier[6].out_features)\n\n# Freeze training for all layers\nfor param in vgg19.features.parameters():\n    param.requires_grad = False\n\n# Modify the classifier to match your task\nnum_features = vgg19.classifier[6].in_features\nnum_classes = len(class_names)  # Replace with the actual number of classes\nclassifier = nn.Sequential(\n    nn.Linear(num_features, 512),  # Add a hidden layer if needed\n    nn.ReLU(),  # Activation function\n    nn.Dropout(0.5),  # Dropout for regularization\n    nn.Linear(512, num_classes)  # Output layer with the desired number of classes\n)\n\nvgg19.classifier[6] = nn.Linear(num_features, num_classes)  # Replace the final layer\n\n# Now, your VGG19 model is ready for fine-tuning with the modified classifier\nprint(vgg19)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pretrained model from pytorch\n# vgg19 not 16\nvgg16 = models.vgg16_bn()\nvgg16.load_state_dict(torch.load(\"/kaggle/input/vgg19bn/vgg19_bn.pth\"))\nprint(vgg16.classifier[6].out_features) # 1000 \n\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nfeatures = list(vgg16.classifier.children())[:-1] # Remove last layer\nfeatures.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*features) # Replace the model classifier\nprint(vgg16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If you want to train the model for more than 2 epochs, set this to True after the first run\nresume_training = True\n\nif resume_training:\n    print(\"Loading pretrained model..\")\n    vgg16.load_state_dict(torch.load('/kaggle/input/vgg16-transfer-learning-pytorch/VGG16_v2-OCT_Retina_half_dataset.pt'))\n    print(\"Loaded!\")","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:18:39.735685Z","iopub.execute_input":"2023-09-08T10:18:39.736475Z","iopub.status.idle":"2023-09-08T10:18:49.195357Z","shell.execute_reply.started":"2023-09-08T10:18:39.73644Z","shell.execute_reply":"2023-09-08T10:18:49.19393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_gpu:\n    vgg16.cuda() #.cuda() will move everything to the GPU side\n    \ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:19:20.298719Z","iopub.execute_input":"2023-09-08T10:19:20.299121Z","iopub.status.idle":"2023-09-08T10:19:20.481202Z","shell.execute_reply.started":"2023-09-08T10:19:20.29909Z","shell.execute_reply":"2023-09-08T10:19:20.480065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_gpu:\n    vgg19.cuda() #.cuda() will move everything to the GPU side\n    \ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Before Training","metadata":{}},{"cell_type":"code","source":"print(\"Test before training\")\neval_model(vgg16, criterion)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:19:36.92398Z","iopub.execute_input":"2023-09-08T10:19:36.924382Z","iopub.status.idle":"2023-09-08T10:20:05.246317Z","shell.execute_reply.started":"2023-09-08T10:19:36.924352Z","shell.execute_reply":"2023-09-08T10:20:05.245093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_model(vgg16)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:20:28.338542Z","iopub.execute_input":"2023-09-08T10:20:28.338965Z","iopub.status.idle":"2023-09-08T10:20:31.981242Z","shell.execute_reply.started":"2023-09-08T10:20:28.338929Z","shell.execute_reply":"2023-09-08T10:20:31.979709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:20:44.036977Z","iopub.execute_input":"2023-09-08T10:20:44.037426Z","iopub.status.idle":"2023-09-08T10:20:44.044822Z","shell.execute_reply.started":"2023-09-08T10:20:44.037385Z","shell.execute_reply":"2023-09-08T10:20:44.04371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n#     since = time.time()\n#     best_model_wts = copy.deepcopy(vgg.state_dict())\n#     best_acc = 0.0\n    \n#     avg_loss = 0\n#     avg_acc = 0\n#     avg_loss_val = 0\n#     avg_acc_val = 0\n    \n#     train_batches = len(dataloaders[TRAIN])\n#     val_batches = len(dataloaders[VAL])\n    \n#     for epoch in range(num_epochs):\n# #         loop = tqdm.notebook.tqdm(train_dataloader)\n# #         loop.set_description(f\"{e+1}/{EPOCHS}\")\n#         print(\"Epoch {}/{}\".format(epoch, num_epochs))\n#         print('-' * 10)\n        \n#         loss_train = 0\n#         loss_val = 0\n#         acc_train = 0\n#         acc_val = 0\n        \n#         vgg.train(True)\n        \n#         for i, data in enumerate(dataloaders[TRAIN]):\n#             if i % 100 == 0:\n#                 print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n                \n#             # Use half training dataset\n#             if i >= train_batches / 2:\n#                 break\n                \n#             inputs, labels = data\n#             inputs, labels = inputs.cuda(), labels.cuda()\n            \n#             optimizer.zero_grad()\n            \n#             outputs = vgg(inputs)\n            \n#             _, preds = torch.max(outputs.data, 1)\n#             loss = criterion(outputs, labels)\n            \n#             loss.backward()\n#             optimizer.step()\n    \n#             loss_train += loss.data\n#             acc_train += torch.sum(preds == labels.data)\n            \n#             del inputs, labels, outputs, preds\n#             torch.cuda.empty_cache()\n        \n#         print()\n#         # * 2 as we only used half of the dataset\n#         avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n#         avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n        \n#         vgg.train(False)\n#         vgg.eval()\n            \n#         for i, data in enumerate(dataloaders[VAL]):\n#             if i % 100 == 0:\n#                 print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n                \n#             inputs, labels = data\n#             inputs, labels = inputs.cuda(), labels.cuda()\n\n#             optimizer.zero_grad()\n            \n#             outputs = vgg(inputs)\n            \n#             _, preds = torch.max(outputs.data, 1)\n#             loss = criterion(outputs, labels)\n            \n#             loss_val += loss.data\n#             acc_val += torch.sum(preds == labels.data)\n            \n#             del inputs, labels, outputs, preds\n#             torch.cuda.empty_cache()\n        \n#         avg_loss_val = loss_val / dataset_sizes[VAL]\n#         avg_acc_val = acc_val / dataset_sizes[VAL]\n        \n#         print()\n#         print(\"Epoch {} result: \".format(epoch))\n#         print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n#         print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n#         print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n#         print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n#         print('-' * 10)\n#         print()\n        \n#         if avg_acc_val > best_acc:\n#             best_acc = avg_acc_val\n#             best_model_wts = copy.deepcopy(vgg.state_dict())\n        \n#     elapsed_time = time.time() - since\n#     print()\n#     print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n#     print(\"Best acc: {:.4f}\".format(best_acc))\n    \n#     vgg.load_state_dict(best_model_wts)\n#     return vgg\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:20:48.187998Z","iopub.execute_input":"2023-09-08T10:20:48.188407Z","iopub.status.idle":"2023-09-08T10:20:48.193665Z","shell.execute_reply.started":"2023-09-08T10:20:48.188376Z","shell.execute_reply":"2023-09-08T10:20:48.192285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:20:50.019138Z","iopub.execute_input":"2023-09-08T10:20:50.019521Z","iopub.status.idle":"2023-09-08T10:20:50.027228Z","shell.execute_reply.started":"2023-09-08T10:20:50.019491Z","shell.execute_reply":"2023-09-08T10:20:50.0262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing\ndef train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(vgg.state_dict())\n    best_acc = 0.0\n    \n    avg_loss = 0\n    avg_acc = 0\n    avg_loss_val = 0\n    avg_acc_val = 0\n    \n    train_batches = len(dataloaders[TRAIN])\n    val_batches = len(dataloaders[VAL])\n    \n    for epoch in tqdm(range(num_epochs), desc = \"Epoch\"):\n#         loop = tqdm.notebook.tqdm(train_dataloader)\n#         loop.set_description(f\"{e+1}/{EPOCHS}\")\n#         print(\"Epoch {}/{}\".format(epoch, num_epochs))\n#         print('-' * 10)\n        \n        loss_train = 0\n        loss_val = 0\n        acc_train = 0\n        acc_val = 0\n        \n        vgg.train(True)\n        \n        for i, data in enumerate(dataloaders[TRAIN]):\n            if i % 100 == 0:\n                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n                \n            # Use half training dataset\n            if i >= train_batches / 2:\n                break\n                \n            inputs, labels = data\n            inputs, labels = inputs.cuda(), labels.cuda()\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n#             train_loss.append(loss.data)\n            loss.backward()\n            optimizer.step()\n            \n            loss_train += loss.data\n            acc_train += torch.sum(preds == labels.data)\n#             train_loss.append(loss_train)\n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        print()\n        # * 2 as we only used half of the dataset\n        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n        train_loss.append(avg_loss)\n        train_acc.append(avg_acc)\n        vgg.train(False)\n        vgg.eval()\n            \n        for i, data in enumerate(dataloaders[VAL]):\n            if i % 100 == 0:\n                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n                \n            inputs, labels = data\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss_val += loss.data\n            acc_val += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        avg_loss_val = loss_val / dataset_sizes[VAL]\n        avg_acc_val = acc_val / dataset_sizes[VAL]\n        val_loss.append(avg_loss_val)\n        val_acc.append(avg_acc_val)\n        \n        print()\n        print(\"Epoch {} result: \".format(epoch + 1))\n        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n        print('-' * 10)\n        print()\n        \n        if avg_acc_val > best_acc:\n            best_acc = avg_acc_val\n            best_model_wts = copy.deepcopy(vgg.state_dict())\n        \n    elapsed_time = time.time() - since\n    print('Completed.')\n    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n    print(\"Best acc: {:.4f}\".format(best_acc))\n    \n    vgg.load_state_dict(best_model_wts)\n    return vgg","metadata":{"execution":{"iopub.status.busy":"2023-09-08T10:21:12.764816Z","iopub.execute_input":"2023-09-08T10:21:12.765219Z","iopub.status.idle":"2023-09-08T10:21:12.78469Z","shell.execute_reply.started":"2023-09-08T10:21:12.765187Z","shell.execute_reply":"2023-09-08T10:21:12.783606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:09:03.533271Z","iopub.execute_input":"2023-09-08T12:09:03.533709Z","iopub.status.idle":"2023-09-08T12:27:01.198295Z","shell.execute_reply.started":"2023-09-08T12:09:03.533676Z","shell.execute_reply":"2023-09-08T12:27:01.1968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_loss)\nprint(val_loss) \nprint(train_acc) \nprint(val_acc) ","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:58:22.947729Z","iopub.execute_input":"2023-09-08T11:58:22.948201Z","iopub.status.idle":"2023-09-08T11:58:23.029246Z","shell.execute_reply.started":"2023-09-08T11:58:22.948163Z","shell.execute_reply":"2023-09-08T11:58:23.028079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg19 = train_model(vgg19, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vgg16.state_dict(), 'VGG16_potato-disease-leaf-dataset.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_model(vgg16, criterion)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T11:58:35.066389Z","iopub.execute_input":"2023-09-08T11:58:35.066822Z","iopub.status.idle":"2023-09-08T11:58:57.69694Z","shell.execute_reply.started":"2023-09-08T11:58:35.066791Z","shell.execute_reply":"2023-09-08T11:58:57.695744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_model(vgg16, num_images=32)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:08:03.729868Z","iopub.execute_input":"2023-09-08T12:08:03.731068Z","iopub.status.idle":"2023-09-08T12:08:13.175794Z","shell.execute_reply.started":"2023-09-08T12:08:03.731023Z","shell.execute_reply":"2023-09-08T12:08:13.174502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_train = []\nacc_train = []\nloss_val = []\nacc_val = []","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:08:20.32644Z","iopub.execute_input":"2023-09-08T12:08:20.326827Z","iopub.status.idle":"2023-09-08T12:08:20.332063Z","shell.execute_reply.started":"2023-09-08T12:08:20.326796Z","shell.execute_reply":"2023-09-08T12:08:20.330974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rechange(model_check):\n    model_new = []\n    for i in model_check:\n        i = i.item()\n        model_new.append(i)\n    return model_new\nloss_train = rechange(train_loss)\nacc_train = rechange(train_acc)\nloss_val = rechange(val_loss)\nacc_val = rechange(val_acc)","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:08:29.57471Z","iopub.execute_input":"2023-09-08T12:08:29.575159Z","iopub.status.idle":"2023-09-08T12:08:29.583547Z","shell.execute_reply.started":"2023-09-08T12:08:29.575125Z","shell.execute_reply":"2023-09-08T12:08:29.582086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_train, label='train')\nplt.plot(loss_val, label='val', color = 'red')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:08:32.73522Z","iopub.execute_input":"2023-09-08T12:08:32.735628Z","iopub.status.idle":"2023-09-08T12:08:33.035033Z","shell.execute_reply.started":"2023-09-08T12:08:32.735596Z","shell.execute_reply":"2023-09-08T12:08:33.034001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(acc_train, label='train')\nplt.plot(acc_val, label='val', color = 'red')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-08T12:08:36.742421Z","iopub.execute_input":"2023-09-08T12:08:36.743557Z","iopub.status.idle":"2023-09-08T12:08:37.04292Z","shell.execute_reply.started":"2023-09-08T12:08:36.743511Z","shell.execute_reply":"2023-09-08T12:08:37.041944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss_model.append(0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_model, label='val')\nplt.title('Model Acc')\nplt.ylabel('Acc')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(loss_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(accuracy, label='train')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}